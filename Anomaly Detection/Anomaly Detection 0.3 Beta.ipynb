{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,191);\">Anomaly Detection</h1>\n",
    "<br/>\n",
    "<p>\n",
    "    In this tutorial we will explore a machine learning use case regarding <strong style=\"color:rgb(0,120,191);\">anomaly detection in bank transactions</strong>. In general, the datasets used in anomaly detection are highly imbalanced, that is, one class is over-representated compared to the other. Our use-case is based on a fraud detection dataset available on <a href=\"https://www.kaggle.com/mlg-ulb/creditcardfraud\">kaggle</a>. <br/> \n",
    "    Here, we will build a predictive model to detect fraudulent cases and detail the ways to approach such a problem, and explain the key metrics of a classification task\n",
    "</p>\n",
    "<p> \n",
    "    <strong>Prerequisites</strong>:<br/> knowledge of following machine learning concepts: feature, label, training data, validation data \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import s3fs\n",
    "import boto3\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import tools\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, roc_curve, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read File from a public bucket in Amazon S3\n",
    "bucket = 'ml-sa-us-east-1'   # \n",
    "prefix = 'sagemaker/fraud-detection'\n",
    "file_name = 'creditcard.csv'\n",
    "src_file = 's3://{}/{}/{}'.format(bucket, prefix, file_name)\n",
    "print('Reading source file  from S3: {}'.format(src_file))\n",
    "df = pd.read_csv(src_file)\n",
    "print('src_file = {} read!'.format(src_file))\n",
    "# df = pd.read_csv('https://s3.amazonaws.com/ml-sa-us-east-1/sagemaker/fraud-detection/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,191);\">1) Exploratory Data Analysis</h1>\n",
    "<h3>Overal shape, missing values, memory usage, distributions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DataFrame of size {} with {} duplicate row(s)'.format(df.shape, len(df.duplicated()[df.duplicated()==True])))\n",
    "def df_mem(dataf):\n",
    "    dataf_mem = np.round(dataf.memory_usage().sum()/1024/1024, 2)\n",
    "    print('Memory usage of the dataframe: {} MB'.format(dataf_mem))\n",
    "df_mem(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate rows\n",
    "print('Among the duplicated rows, {} are class 0, and {} are class 1'.format(df.loc[df.duplicated()==True, 'Class'].value_counts()[0],\n",
    "                                                                             df.loc[df.duplicated()==True, 'Class'].value_counts()[1]))\n",
    "print(\"Let's remove the duplicates\")\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df_mem(df)\n",
    "print('DataFrame of size {} with {} duplicate row(s)'.format(df.shape, len(df.duplicated()[df.duplicated()==True])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values for each column: {}\\n'.format(list(df.isnull().sum()) ))\n",
    "print('Distinct values: ')\n",
    "print([ (col, len(df[col].unique())) for col in df.columns ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.50, 0.95]).loc[['min', '50%','95%','max', 'mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    We don't know what the attributes stand for, but they have various ranges as well as various variances. \n",
    "    <br/>In the modelling part, the numeric inputs will be rescaled to be used with algorithms which compute distances (KNN, K-means, ...) <br/>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluating statistic correlation of the attributes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [go.Heatmap(z=df.corr().values, x=df.corr().index, y=df.corr().index.tolist())]\n",
    "layout = go.Layout(title='Correlation heatmap', width=800, height=400)\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    From the heatmap, we deduce:\n",
    "    <ul>\n",
    "        <li>V7 and amount are positively correlated with a correlation score of 0.4</li>\n",
    "        <li>The other attributes are poorly correlated between each other</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = tools.make_subplots(rows=1, cols=1, print_grid=False)\n",
    "fig['layout'].update(title='amount VS V7',\n",
    "                         showlegend=False, width = 900, height=350,\n",
    "                         margin=go.layout.Margin(l=50, r=50, b=50, t=50, pad=10))\n",
    "data = [ go.Scattergl(x=df['V7'], y=df['Amount'], name='Amount VS V7', mode='markers') ]    \n",
    "# fig.append_trace(data, 1, 1)\n",
    "# data = go.Scattergl(x=df['attribute3'], y=df['attribute9'], name='attr9 VS attr3', mode='markers',\n",
    "# #                    xaxis=dict(title='attribute3'), yaxis=dict(title='attribute9')\n",
    "#                    ) \n",
    "fig = go.Figure(data)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plotting the distributions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [go.Histogram(x = df['Class'])]\n",
    "xtick_labs = ['No Fraud', 'Fraud Occured']\n",
    "layout = go.Layout(title='Distribution of the fraud label', \n",
    "                   showlegend=False, \n",
    "                   width = 800,\n",
    "                   height=350, \n",
    "                   margin=go.layout.Margin(l=50, r=50, b=50, t=50, pad=10), \n",
    "                   xaxis=dict(autorange=False, range=[-1,2], showticklabels=True, ticktext=xtick_labs, tickvals=[0,1]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The dataset is <strong>severly imbalanced</strong>! Out of 283726 samples, 283253 are negative (i.e. no fraud) and only  473 are positive (i.e. fraud occured). <br/>The ratio is 599:1\n",
    "    <br/> Let's plot the distribution of the features before discussing how we will work with this imbalanced dataset.    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nCols = 5\n",
    "fig = tools.make_subplots(rows=1, cols=nCols, print_grid=False)\n",
    "for idx, col in enumerate(df.columns[:nCols],1):\n",
    "    data = go.Box(y=df[col], name = col)\n",
    "    fig.append_trace(data,1,idx)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rows with fraud occurences in df_fail\n",
    "df_fail = df.loc[df['Class']==1]\n",
    "\n",
    "# draw a box plot\n",
    "def draw_box(colName, position):\n",
    "    fig = tools.make_subplots(rows=1, cols=5, print_grid=False)\n",
    "    data = go.Box(y=df[col], name = col)\n",
    "    fig.append_trace(data,1,position)\n",
    "    return fig    \n",
    "    \n",
    "# draw a scatter plot \n",
    "def draw_scatter(colName):\n",
    "    fig = tools.make_subplots(rows=1, cols=1, print_grid=False)\n",
    "    fig['layout'].update(title='fraud = f(' + colName + ')',\n",
    "                         showlegend=False, width = 900, height=350,\n",
    "                         margin=go.layout.Margin(l=50, r=50, b=50, t=50, pad=10))\n",
    "    data = go.Scattergl(x=df[colName], y=df['Class'], name=colName, mode='markers', \n",
    "                        marker=dict(color=df['Class']))     \n",
    "    fig.append_trace(data, 1, 1)\n",
    "    iplot(fig)\n",
    "    return fig\n",
    "\n",
    "def draw_distrib(colName):\n",
    "    fig = tools.make_subplots(rows=1, cols=2, print_grid=False)\n",
    "    fig['layout'].update(title='Distribution of the column \"' + '<b>' + colName + '</b>' + '\"''<br> <span style=\"color:rgb(0,120,191);\"> OVER ALL SAMPLES</span>                            <span style=\"color:rgb(255,140,0);\"> OVER SAMPLES WHERE FRAUD=1</span>',\n",
    "                         showlegend=False, width = 900, height=350,\n",
    "                         margin=go.layout.Margin(l=50, r=50, b=50, t=50, pad=10))\n",
    "    data = go.Histogram(x=df[colName], autobinx=True, name=colName)    \n",
    "    fig.append_trace(data, 1, 1)\n",
    "    data = go.Bar(x=df_fail[colName].value_counts().index, y=df_fail[colName].value_counts().values, name=colName) \n",
    "#     data = go.Histogram(x=df_fail[colName], autobinx=True, name=colName) \n",
    "    fig.append_trace(data, 1, 2)\n",
    "    iplot(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns[:5]:\n",
    "    draw_distrib(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns[5:10]:\n",
    "    draw_distrib(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns[10:15]:\n",
    "    draw_distrib(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns[15:20]:\n",
    "    draw_distrib(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df.columns[20:25]:\n",
    "    draw_distrib(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns[25:]:\n",
    "    draw_distrib(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <ul>\n",
    "    <li>Fraudulent operations occur at any time, they do not exibit any periodic pattern</li>    \n",
    "    <li>V1 distribution is skewed, it is concentrated in the range [-10, 2], plus some outliers in the range [-55, -10[ which don't correlate with the fraudulent cases<br/>\n",
    "        <li>V24 and V26 look like gaussian mixtures, and exhibit some outliers which don't correlate with the fraud cases</li>\n",
    "    <li>All other attributes V2, V3 until V23 (included), V25, V27, V28 show a gaussian distribution plus some outliers which don't correlate with the fraudulent cases </li>    \n",
    "    <li>95% of the amount is below 365 (Unit), nevertheless fraudulent cases occur for all amounts</li>            \n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Outlier removal</h3>\n",
    "<p>\n",
    "As the outlier do not seem to be correlated with the failure occurences, they will be removed from the dataset\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outlier(dataf, cols, quant, thd, label):\n",
    "    \"\"\"\n",
    "    dataf (pandas dataframe)\n",
    "    cols (list) of the columns of interest of the dataframe\n",
    "    quant (list) of the quantiles to extract\n",
    "    thd (float)  when the relative gap (Max-99%)/Max exceeds this thd we cap the values\n",
    "    \"\"\"\n",
    "    perc = df.describe(percentiles=quant).loc[['99%','max']]\n",
    "    rel_Err = (perc.loc['max']-perc.loc['99%']) / perc.loc['max']\n",
    "    cols_to_cap = rel_Err[rel_Err > thd]\n",
    "    cols_to_cap = cols_to_cap[cols_to_cap.index.str.contains('attribute')].index.tolist()\n",
    "\n",
    "    for col in cols_to_cap:\n",
    "        capped_val = perc.loc['99%',col]\n",
    "        dataf.loc[dataf[col]>capped_val, col] = capped_val\n",
    "    return dataf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cap_outlier(df, df.columns.str.contains('attribute').tolist(), [0.99, 1], 0.95, 'failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rescaling numeric features (i.e. attribute N)</h3>\n",
    "<p>\n",
    "    MinMaxScaler will be used. \n",
    "    The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_scale = df.columns.drop(['Time', 'Class']).tolist()\n",
    "df = scale_df(df, col_to_scale, False)\n",
    "df_mem(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dimensionnality reduction techniques and clustering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(dataf, columns, Bprint):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    scaled_val = min_max_scaler.fit_transform(dataf[columns])\n",
    "    if Bprint:\n",
    "        print('min_max_scaler.scale_ = {}'.format(min_max_scaler.scale_))\n",
    "    for idx, col in  enumerate(columns): \n",
    "        dataf[col] = scaled_val[:,idx]\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPCA(dataf, label, method,n_comp, leg0, leg1, mean_rem):\n",
    "    \"\"\"\n",
    "    df is the source dataframe\n",
    "    label is a string with the name of the label\n",
    "    method is a string indicating which part of the dataframe to keep, method='us' for an undersampled version, method='all' for the whole dataset\n",
    "    n_comp is an integer, number of components of the PCA   \n",
    "    leg0 is a string for the legend of the Negative class\n",
    "    leg1 is a string for the legend of the Positive class\n",
    "    mean_rem is a boolean value, True for removing mean of the columns, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    df_ano = dataf[dataf[label]==1]\n",
    "    if method=='us':\n",
    "        X = dataf.loc[dataf[label]==0]    # Keep only negative samples\n",
    "        X = X.sample(n=df_ano.shape[0])\n",
    "        X = pd.concat([X[:df_ano.shape[0]], df_ano], axis=0)\n",
    "    else:\n",
    "        X = dataf\n",
    "    \n",
    "   \n",
    "    # PCA Implementation\n",
    "    t0 = time.time()\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    X_red = pca.fit_transform(X)\n",
    "    print('PCA explained variance = {:1.2f}% {:1.2f}%'.format(pca.explained_variance_ratio_[0]*100, pca.explained_variance_ratio_[1]*100))\n",
    "    print('PCA explained variance = {:1.6f} {:1.6f}'.format(pca.explained_variance_[0], pca.explained_variance_[1]))\n",
    "    print('singular_values_ = {:1.6f} {:1.6f}'.format(pca.singular_values_[0], pca.singular_values_[1]))\n",
    "    print('mean_ = {:1.6f} {:1.6f}'.format(pca.mean_[0], pca.mean_[1]))\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    idx_class0 = X[X[label]==0].index     # Get index of the Negative class\n",
    "    idx_class1 = X[X[label]==1].index     # Get index of the Positive class\n",
    "    c0_iloc = [ X.index.get_loc(x) for x in idx_class0 ]  # Transform index into iloc(index)\n",
    "    c1_iloc = [ X.index.get_loc(x) for x in idx_class1 ]\n",
    "    \n",
    "    X = X.drop(label,axis=1)\n",
    "    if mean_rem==True:\n",
    "        X = X-np.mean(X,axis=0)    \n",
    "\n",
    "    lay = go.Layout(xaxis=dict(title='Principal Component 1'), yaxis=dict(title='Principal Component 2'), title='PCA Decomposition')\n",
    "    data = [ go.Scattergl(x=X_red[c0_iloc,0], y=X_red[c0_iloc,1], mode='markers', marker=dict(color='blue'), name=leg0)]\n",
    "    data.append( go.Scattergl(x=X_red[c1_iloc,0], y=X_red[c1_iloc,1], mode='markers', marker=dict(color='red'), name=leg1) )\n",
    "    fig = go.Figure(data=data,layout=lay)\n",
    "    iplot(fig)\n",
    "    \n",
    "    return fig, X_red, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, X_red, pca = plotPCA(df, label='Class', method='all',n_comp=2, leg0='No Fraud', leg1='Fraud', mean_rem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The data is quite complex, it seems it cannot be separated along any of the 2 principal components. Nevertheless Fraudulent data are much widely spread along the Y axis than non fraudulent data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,191);\">2) Some Feature Engineering</h1>\n",
    "<ul>\n",
    "From the observations made before,<br/><br/>\n",
    "    <li>The range of the numeric features are various, thus we will rescale them to fit in the range [0,1]</li>\n",
    "    <li>As failure occurences do not exhibit any temporality, we will be able to shuffle the dataset as needed. Plus, the date won't be used in the features </li>\n",
    "    <li>Attributes V2, V3, V28 have several levels with low frequencies, and chances are they wouldn’t be available in test set, so these attributes will be binned (i.e. partitioned into intervals) </li>\n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding Principal Component to the dataframe</h3>\n",
    "<p>\n",
    "    MinMaxScaler will be used. \n",
    "    The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_red = pca.fit_transform(df)\n",
    "df['PC1'] = X_red[:,0]\n",
    "df['PC2'] = X_red[:,1]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,191);\">3) The Testing Procedure</h1>\n",
    "<p>\n",
    "    As said before, the dataset provided by the customer is highly imbalanced, <strong>99.8% of the values belong to class 0 (No fraud)</strong> and <strong>0.2% to the positive class (fraud occured)</strong>. In absolute number we have  <strong>283253 negative samples and 473 positive samples.</strong>\n",
    "</p>\n",
    "<p>\n",
    "    To evaluate the performance of the classifiers we design, Ideally, we need a test set which was not used to train the classifiers. To build such a test set from the sample dataset provided, we will divide the dataset into 2 parts which keep the same class imbalance ratio. \n",
    "</p> \n",
    "    \n",
    "<p>\n",
    "    While this procedure will allow us to leverage the test set for detecting overfitting issues, we must keep in mind that we will be \"losing\" valuable information. So, <strong>at first</strong>, we will train classifiers on the train set and validate them on the test set.\n",
    "</p>\n",
    "<p>We could also try cross-validation, but for now let's stick to the train/test split! \n",
    "</p>    \n",
    "\n",
    "<p>\n",
    "    In the next sections, we will define functions for dividing dataset, feature selection, model selection and evaluation ...\n",
    "    Then we will assemble all these into a nice pipeline\n",
    "</p>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dividing the dataset into train & test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set(dataf, test_siz):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataf.drop(columns='Class'), dataf['Class'], test_size=test_siz, random_state=42, stratify=dataf['Class'])\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Selecting features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(dataf, mode):\n",
    "    if mode == 'not_pca':\n",
    "        feats = dataf.columns[~dataf.columns.str.contains('PC')].drop(['Time', 'Class']).tolist()       \n",
    "    elif isinstance(mode, dict):\n",
    "        feats = mode['attr']   \n",
    "    elif mode == 'all':\n",
    "        feats = df.columns.drop(['Time', 'Class']).tolist()\n",
    "    elif mode == 'pca':\n",
    "        feats = ['PC1', 'PC2']        \n",
    "    print('List of {} features: {}'.format(len(feats),feats))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,191);\">4) Methods for working with an imbalanced dataset</h1>\n",
    "<p>\n",
    "A classifier is built to minimize classification errors. Since the probability of instances belonging to the majority class is very high in this imbalanced data set, the algorithms will tend to classify unseen observations to the majority class. In our case, with a classifier which systematically predict the majority class (i.e. no failure), we could easily achieve 99.8% accuracy!\n",
    "</p>\n",
    "<p>\n",
    "    To address imbalanced dataset, we can act on 2 approaches\n",
    "    <ul>\n",
    "        <li><strong>algorithm approach</strong>: basically, ML algorithms penalize False Positive (FP) and False Negative (FN) equally, but implementation of some algorithms provide a hyper-parameter which weights each class example proportionally to the inverse of its frequency</li><br/>\n",
    "        <li><strong>data approach</strong>: it consists of resampling the data so as to lower the imbalance ratio, we can use under-sampling or/and over-sampling</li>.<ul>\n",
    "            <li>Under-sampling balances the dataset by reducing the size of the abundant class. But, since it is removing observations from the original data set, it might discard useful information</li>\n",
    "        <li>Over-sampling balances the dataset by increasing the size of rare samples. No information from the original training set is lost, as all observations from the minority and majority classes are kept. But it is prone to overfitting</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Now let's implement Under-sampling and Over-sampling technics with the <strong>imbalance-learn</strong> package before discussing the ML algorithms we'll use\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Under-sampling the data</h3>\n",
    "<p>\n",
    "    Let's wrap 3 under_sampling methods in a custom function:\n",
    "    <ul>\n",
    "        <li>RandomUnderSampler is a naive way which randomly selects a given number of samples by the targetted class</li>\n",
    "        <li>EditedNearestNeighbours removes samples of the majority class for which their class differ from the one of their nearest-neighbors</li>\n",
    "        <li>NearMiss-1 selects samples from the majority class for which the average distance of the k nearest samples of the minority class is the smallest</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_samp(method, seed, r_Majas_over_min, x_train, y_train):\n",
    "    \"\"\" \n",
    "    dataf (dataframe) on which the undersampling is to be applied\n",
    "    method (string) chooses on of the under-sampling method available in the imbalanced-learn package\n",
    "    seed (int) controls the initialization of the random generator\n",
    "    r_Majas_over_min (float) is the ratio of the nb_majority_class after resampling/nb_minority_class \n",
    "    \"\"\"\n",
    "    if method == 'random':\n",
    "        us = under_sampling.RandomUnderSampler(ratio=r_Majas_over_min, return_indices=True, random_state=seed, replacement=False)\n",
    "    elif method == 'ENN': \n",
    "        us = under_sampling.EditedNearestNeighbours(sampling_strategy=r_Majas_over_min, return_indices=True, random_state=seed, n_neighbors=3, kind_sel='all', n_jobs=1)\n",
    "    elif method =='NM':\n",
    "        us = under_sampling.NearMiss(sampling_strategy=r_Majas_over_min, return_indices=True, random_state=seed, version=1, n_neighbors=3, n_neighbors_ver3=3, n_jobs=1)        \n",
    "    # Resample data\n",
    "    print('under-sampling data!')\n",
    "    x_train, y_train, ind_us = us.fit_resample(x_train, y_train)\n",
    "    print('After under_sampling x_train.shape = {}, y_train.shape= {}'.format(x_train.shape, y_train.shape))  \n",
    "    \n",
    "    return us, x_train, y_train, ind_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Over-sampling the data</h3>\n",
    "<p>\n",
    "    Whith over-sampling, more information is retained since we don't delete any rows unlike in random undersampling.\n",
    "We will take more time to train since no rows are eliminated as previously stated. <br/>\n",
    "    Let's wrap 3 over_sampling methods in a custom function:\n",
    "    <ul>\n",
    "        <li>RandomOverSampler is a naive way which generates new samples by randomly sampling with replacement the current available samples in the minority classes</li>\n",
    "        <li>SMOTE generates new samples by creating synthetic samples from the minor class instead of creating copies. SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points</li>\n",
    "        <li>ADASYN also creates synthetic data points but, for the new data points to be realistic, ADASYN adds a small error to the data points to allow for some variance</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_samp(method, seed, r_Maj_over_minas, x_train, y_train):\n",
    "    \"\"\" \n",
    "    method (string) chooses on of the over-sampling method available in the imbalanced-learn package\n",
    "    seed (int) controls the initialization of the random generator\n",
    "    r_Maj_over_minas (float) is the ratio of the nb_majority_class/nb_minority_class after resampling \n",
    "    \"\"\"\n",
    "    if method == 'random':\n",
    "        os = over_sampling.RandomOverSampler(sampling_strategy=r_Maj_over_minas, random_state=seed, replacement=False)\n",
    "    elif method == 'SMOTE': \n",
    "        os = over_sampling.SMOTE(sampling_strategy=r_Maj_over_minas, random_state=seed, k_neighbors=5, n_jobs=1)\n",
    "    elif method =='ADASYN':\n",
    "        os = over_sampling.ADASYN(sampling_strategy=r_Maj_over_minas, random_state=seed, n_neighbors=5, n_jobs=1)\n",
    "\n",
    "    # Resample data\n",
    "    print('over-sampling data!')\n",
    "    n_fail_y_train = len(y_train[y_train==1])   \n",
    "    print('Before over_sampling x_train.shape = {}, y_train.shape= {}'.format(x_train.shape, y_train.shape))\n",
    "    x_train, y_train = os.fit_resample(x_train, y_train)\n",
    "    print('After over_sampling x_train.shape = {}, y_train.shape= {}'.format(x_train.shape, y_train.shape))\n",
    "                                  \n",
    "    return os, n_fail_y_train, x_train, y_train                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Choosing the classification algorithm</h2>\n",
    "<p>\n",
    "    We'll give a try at Logistic regression & Support vector Machines, as these two support hyperparameters for balancing the dataset.<br/> \n",
    "    We will aso try Naïve Bayes, Random Forest, Gradient boosting and a feed-forward Neural network .\n",
    "    Let's wrap the model classifier selection into a custom function which selects either of these\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clf(method, seed, feat_list):\n",
    "    \"\"\"\n",
    "    method (string) specifying the type of classifier to use\n",
    "    seed (int) specifying the initialisation state of the random generator\n",
    "    \"\"\"\n",
    "    if method == 'log_reg':\n",
    "        clf = LogisticRegression(class_weight=None, random_state=seed, solver='saga', max_iter=1000, multi_class='ovr')\n",
    "    elif method == 'log_reg_w':\n",
    "        clf = LogisticRegression(class_weight='balanced', random_state=seed, solver='saga', max_iter=1000, multi_class='ovr')\n",
    "    elif method == 'rCV_log_reg_w':\n",
    "        clf1 = LogisticRegression(class_weight='balanced', random_state=seed, solver='saga', max_iter=1000, multi_class='ovr')\n",
    "        log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "        clf = RandomizedSearchCV(clf1, log_reg_params, n_iter=4, cv=3, scoring='precision')  #'average_precision', 'precision'\n",
    "\n",
    "    elif method == 'GNB':\n",
    "        clf = GaussianNB()\n",
    "    elif method == 'CNB':\n",
    "        clf = ComplementNB()        \n",
    "    elif method == 'SVM':\n",
    "        clf = LinearSVC(loss='squared_hinge', dual=False, tol=0.0001, multi_class='ovr', class_weight=None, random_state=seed, max_iter=1000)\n",
    "    elif method == 'SVM_w':\n",
    "        clf = LinearSVC(loss='squared_hinge', dual=False, tol=0.0001, multi_class='ovr', class_weight='balanced', random_state=seed, max_iter=1000)\n",
    "    elif method == 'RF':\n",
    "        clf = RandomForestClassifier(n_estimators=100, max_depth=None, max_features=None, max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                                     random_state=seed, verbose=0, warm_start=False, class_weight=None)\n",
    "    elif method == 'RF_w':\n",
    "        clf = RandomForestClassifier(n_estimators=100, max_depth=None, max_features=None, max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                                     random_state=seed, verbose=0, warm_start=False, class_weight='balanced')        \n",
    "    elif method == 'xgb':\n",
    "        clf = XGBClassifier(objective='binary:logistic', scale_pos_weight=1)\n",
    "    elif method == 'rCV_xgb':\n",
    "        clf1 = XGBClassifier(objective='binary:logistic')      \n",
    "        xgb_params = {\"eta\": [0.01,0.1, 1], 'scale_pos_weight': np.linspace(0.5,141626/237, 10), 'max_depth': [6, 10, 15, 20]}\n",
    "        clf = RandomizedSearchCV(clf1, xgb_params, n_iter=4, cv=3, scoring='precision')        \n",
    "    elif method == 'MLP':\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(len(feat_list), len(feat_list)), activation='tanh', solver='adam', alpha=1e-5, batch_size='auto', max_iter=200, shuffle=True, random_state=seed)\n",
    "    elif method == 'bagMLP':\n",
    "        clf1 = MLPClassifier(hidden_layer_sizes=(len(feat_list), len(feat_list)), activation='tanh', solver='adam', alpha=1e-5, batch_size='auto', max_iter=200, shuffle=True, random_state=seed)\n",
    "        clf = BaggingClassifier(clf1)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def ens_clf(ml_clf, seed, feat_list, x_train, y_train, div_factor):\n",
    "    \"\"\"\n",
    "    ml_clf (string, list or dictionnary) if list, create ensemble of classifiers all trained of the same dataset\n",
    "                                         if dictionnary, create an ensemble of classifiers each classifier is trained\n",
    "                                         on a different portion of the negative samples  concatenated with all the positive samples\n",
    "                                         if list, just get the corresponding classifier from the select_clf function defined above\n",
    "    div_factor (integer), partition negative samples of train set into (div_factor*number_of_minority samples) parts \n",
    "    \"\"\"\n",
    "    # Select a classifier\n",
    "    if isinstance(ml_clf,list):  # Ensemble classifiers\n",
    "        print('ml_clf is a list, Ensembling classifiers')\n",
    "        list_estimators = []\n",
    "        for idx, cur_clf in enumerate(ml_clf):\n",
    "            clf_temp = select_clf(cur_clf, seed, feat_list)       \n",
    "            list_estimators.append((cur_clf, clf_temp))\n",
    "        clf = VotingClassifier(estimators=list_estimators, voting='hard')\n",
    "        \n",
    "    elif isinstance(ml_clf, dict):\n",
    "        print('ml_clf is a dict', 'Ensembling classifiers trained on different samples of the train set')\n",
    "#   Divide train set into N * n_fail_y_train parts. Each training set must contain all the samples of the >=0 class\n",
    "#  and N * n_fail_y_train different samples of the <=0 class        \n",
    "        x_train_pos = x_train[y_train==1]\n",
    "        x_train_neg = x_train[y_train==0]\n",
    "        divide_factor = div_factor*x_train_pos.shape[0]\n",
    "        print('Divide factor = {:.1f}'.format(divide_factor))\n",
    "        nbClass2Train = x_train_neg.shape[0] // divide_factor + 1\n",
    "        print('Number of classifiers to train = {:.2f}'.format(nbClass2Train))\n",
    "#         pred_probs = np.zeros((divide_factor+n_fail_y_train, nbClass2Train))\n",
    "        list_estimators = []    \n",
    "        all_estim = list(ml_clf.keys())\n",
    "        for idx in range((nbClass2Train)):\n",
    "            cur_clf = all_estim[idx % len(all_estim)]\n",
    "            if idx == nbClass2Train :\n",
    "                x_train_cur = x_train_neg[idx*divide_factor:]\n",
    "            else:\n",
    "                x_train_cur = x_train_neg[idx*divide_factor: (idx+1)*divide_factor] \n",
    "            y_train_cur = np.zeros((x_train_cur.shape[0],))  \n",
    "            x_train_cur = np.concatenate((x_train_cur, x_train_pos), axis=0)\n",
    "            y_train_cur = np.concatenate((y_train_cur, np.ones((x_train_pos.shape[0],))), axis=0)             \n",
    "            clf_temp = select_clf(cur_clf, seed, feat_list)\n",
    "            print('Training Classifier {}: {}'.format(str(idx+1), cur_clf))\n",
    "            clf_temp.fit(x_train_cur, y_train_cur)\n",
    "            list_estimators.append((cur_clf+str(idx), clf_temp))       \n",
    "        clf = VotingClassifier(estimators=list_estimators, voting='soft')   \n",
    "        clf.estimators_ = [estim[1] for estim in list_estimators]\n",
    "        clf.le_ = LabelEncoder().fit(y_train_cur)\n",
    "        clf.classes_ = clf.le_.classes_\n",
    "        \n",
    "    else:\n",
    "        clf = select_clf(ml_clf, seed, feat_list)\n",
    "\n",
    "    if not isinstance(ml_clf, dict):        \n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "    if ml_clf == 'rCV_log_reg_w':\n",
    "        clf = clf.best_estimator_\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The problem with accuracy</h2>\n",
    "<p>\n",
    "    First, let's remind the definition of a confusion matrix:<br/>\n",
    "    Class 0 (no failure) is called the negative class, class 1 (failure occured) is called the positive class.<br/>\n",
    "    Given a new samples, a trained classifier will predict either class 0 or class 1. Now the 4 cases described below can happen\n",
    "    <img src=\"confusion_matrix.png\" alt=\"Image of confusion_matrix\" width=\"300px\"/>\n",
    "</p>\n",
    "<p>\n",
    "    False positive (FP) is when our classifier predicts a failure whereas there is none, at worst the consequences would be a useless replacement of the machine, so it would cost time and money. \n",
    "</p>\n",
    "<p>On the other hand, false negative (FN) occures when the classifier predicts no failure whereas a failure occures, the consequences of such misclassification (FN) is to have to replace the machine (long?) after the default has occured, which would cost not only time of replacement and the cost of the machine but also everything that was manufactured/controlled by the device is likely to be wasted. \n",
    "</p>\n",
    "<p>So in this sense, FN may be worse than FP. Anyway we want our classifier to minimize the false positive (FP) and false negative (FN) as well.\n",
    "</p> \n",
    "<p>\n",
    "        As stated before, the high imbalance ratio of this dataset makes it impossible to evaluate the performance of our models with the accuracy metric. <br/>\n",
    "    The accuracy is defined as such:  <strong>accuracy = (TP +TN) / (TP + TN + FP + FN)</strong> <br/>\n",
    "    In our case, with a classifier which systematically predict the majority class (i.e. no failure), we could easily achieve 99.9% accuracy!\n",
    "</p><br/>\n",
    "\n",
    "<h2>Choosing the right metrics</h2>\n",
    "\n",
    "<ul> \n",
    "    <li><strong>Precision</strong> measures the fraction of examples classiﬁed as positive that are truly positive: <strong>precision = TP / (TP + FP) </strong></li><br/>\n",
    "    <li><strong>Recall</strong> (also called True Positive Rate) evaluates the ability of the classifier to label positive exemples correctly: <strong>recall = TP / (TP + FN)</strong></li><br/>\n",
    "    <li><strong>Fscore</strong> is the harmonic mean of precision and recall: <strong>F-Score = 2 x precision x recall / (precision + recall)</strong></li><br/>\n",
    "    <li><strong>ROC curve</strong> plots the TPR (True Positive Rate, also called Recall) as a function of the FPR (False Positive Rate). \n",
    "While the TPR measures the fraction of positive examples that are correctly labeled, the FPR measures the fraction of negative examples that are misclassified as positive: <strong>FPR = FP / (FP + TN)</strong>\n",
    "    </li><br/>\n",
    "    <li><strong>PR curve</strong> plots precision as a function of recall. In our case, the number of negative examples greatly exceeds the number of positives examples. Consequently, a large change in the number of false positives can lead to a small change in the false positive rate used in ROC analysis. Precision, on the other hand, by comparing false positives to true positives rather than true negatives, captures the effect of the large number of negative examples on the algorithm’s performance\n",
    "    </li>    \n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, y_probas):\n",
    "    dic_metrics = {}\n",
    "    dic_metrics['accuracy'] = np.round(100*accuracy_score(y_true, y_pred), 2) \n",
    "    dic_metrics['precision'] = np.round(100*precision_score(y_true, y_pred), 2) \n",
    "    dic_metrics['recall'] = np.round(100*recall_score(y_true, y_pred), 2)\n",
    "    dic_metrics['f1_score'] = np.round(100*f1_score(y_true, y_pred), 2)\n",
    "    \n",
    "    if not isinstance(y_probas, str):\n",
    "        precision_vec, recall_vec, thresholds_vec = precision_recall_curve(y_true, y_probas)     \n",
    "        dic_metrics['avg_precision'] = np.round(100*average_precision_score(y_true, y_probas), 2)\n",
    "        trace = go.Scattergl(x=recall_vec, y=precision_vec, \n",
    "                    mode='lines',\n",
    "                    line=dict(width=2),\n",
    "                    name='Precision-Recall curve')\n",
    "    else:\n",
    "        trace = go.Scattergl(x=[0], y=[0])\n",
    "        dic_metrics['avg_precision'] = None\n",
    "    return dic_metrics, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "#     title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPRCurve(traces_dict, perfos_dict):\n",
    "    \n",
    "    layout = go.Layout(\n",
    "#             title='Precision-Recall example: AUC={0:0.2f}'.format(average_precision),\n",
    "            title='Precision-Recall train and test set',\n",
    "            xaxis=dict(title='Recall'),\n",
    "            yaxis=dict(title='Precision'),\n",
    "            height=350, width=800)\n",
    "\n",
    "    traces_to_plot = []    \n",
    "    for key, val in traces_dict.items():\n",
    "        if not isinstance(val, str):\n",
    "            traces_to_plot.append(val)\n",
    "    \n",
    "    fig = go.Figure(data=traces_to_plot, layout=layout)\n",
    "    iplot(fig)    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,191);\">5) Creating pipelines</h1>\n",
    "<p>\n",
    "    Let's write a code which combines every function we built\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPipeline(dataf, seed_used, test_size, feat_sel_mode, cross_val_meth, under_samp_meth, \n",
    "               over_samp_meth, r_Majas_over_min, r_Maj_over_minas, ml_clf='log_reg_w', plot_prc=True, plot_cfm=True):\n",
    "    \"\"\"\n",
    "    dataf (DataFrame) full dataset before splitting into train/test sets\n",
    "    seed_used (integer) controls the initialization state of the random generator\n",
    "    test_size (float) in the range [0.0, 1.0]\n",
    "    feat_sel_mode (string)  'binned' or 'not_binned'\n",
    "    under_samp (string)  method used for under_sampling,  '' for no under_sampling\n",
    "    over_samp (string) method used for over_sampling,  '' for no over_sampling\n",
    "    r_Majas_over_min (float) is the ratio of the nb_majority_class after resampling/nb_minority_class, considered only if under_samp_meth != None\n",
    "    r_min_over_Maj (float) is the ratio of the nb_majority_class/nb_minority_class after resampling, considered only if over_samp_meth != None \n",
    "    ml_clf  if (string) then: classifier to use. \n",
    "            If (list of strings) then: list of the classifiers to use as an ensemble algorithm\n",
    "            If dictionnary, keys are string describing the 'algo', values are float describing the number of this type of classifier/\n",
    "    plot_prc (boolean) True to plot Precision-Recall curve\n",
    "    plot_cfm (boolean)  True to plot confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Divide dataset into train and test sets of equal number, and preserve class imbalance ratio\n",
    "    x_train, x_test, y_train, y_test = create_test_set(dataf, test_size)  # stratified division\n",
    "    n_fail_df = dataf.loc[dataf.Class==1].shape[0]   # nb anomaly in df\n",
    "    n_fail_y_train = len(y_train[y_train==1])   # nb of anomaly in the minority class\n",
    "    print('Number of anomalies in train set = {}\\nNumber of regular samples in train set = {}'.format(n_fail_y_train, x_train.shape[0]-n_fail_y_train ))\n",
    "    print('Dataset divided into train and test')\n",
    "    print('x_train.shape = {}\\nx_test.shape = {}\\ny_train.shape = {}\\ny_test.shape = {}\\n'.format(x_train.shape, x_test.shape, y_train.shape, y_test.shape))\n",
    "\n",
    "    # Select features\n",
    "    feat_list = select_features(df, mode=feat_sel_mode)\n",
    "    x_train, x_test = x_train[feat_list], x_test[feat_list]\n",
    "    print('After selecting features, x_train.shape = {}\\nx_test.shape = {}\\n'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "    out = {}\n",
    "#     Cross-validation\n",
    "    traces_dict = {}\n",
    "    perf_dict = {}\n",
    "    if cross_val_meth == 'cv':\n",
    "        skf = StratifiedKFold(5, shuffle=False, random_state=seed_used)\n",
    "        perf_cv, traces_cv = {} , {}  # Initialize dictionnary for storing the performance metrics\n",
    "        acc_list, rec_list, prec_list, f1_score_list = [], [], [], []\n",
    "        for train_index, test_index in skf.split(x_train, y_train):\n",
    "            x_train_cv, y_train_cv = x_train.iloc[train_index], y_train.iloc[train_index]\n",
    "            x_val_cv, y_val_cv =  x_train.iloc[test_index], y_train.iloc[test_index]  \n",
    "            if under_samp_meth != None:\n",
    "                us, x_train_cv, y_train_cv, ind_us = under_samp(method, seed, r_Maj_over_minas, x_train_cv, y_train_cv)\n",
    "                out['us'] = us\n",
    "            if over_samp_meth != None:\n",
    "                os, n_fail_y_train, x_train_cv, y_train_cv = over_samp(method, seed, r_Maj_over_minas, x_train_cv, y_train_cv)            \n",
    "                out['os'] = os\n",
    "            clf = ens_clf(ml_clf, seed_used, feat_list, x_train_cv, y_train_cv, 100)\n",
    "            clf.fit(x_train_cv, y_train_cv)\n",
    "            y_pred_val_cv = clf.predict(x_val_cv)\n",
    "            try:\n",
    "                y_proba_val_cv = clf.predict_proba(x_val_cv)[:,1]\n",
    "            except:\n",
    "                y_proba_val_cv = 'Classifier has no \"predict_proba\" method'\n",
    "            perf_cv, traces_cv = compute_metrics(y_val_cv, y_pred_val_cv, y_proba_val_cv)\n",
    "            acc_list.append(perf_cv['accuracy']), rec_list.append(perf_cv['precision']), \n",
    "            prec_list.append(perf_cv['recall']), f1_score_list.append(perf_cv['f1_score'])\n",
    "        mean_cv = {}; std_cv = {}\n",
    "        mean_cv['accuracy'] = np.mean(acc_list) ; std_cv['accuracy'] = np.std(acc_list)\n",
    "        mean_cv['precision'] = np.mean(prec_list)   ; std_cv['precision'] = np.std(acc_list)        \n",
    "        mean_cv['recall'] = np.mean(rec_list)   ; std_cv['recall'] = np.std(acc_list)\n",
    "        mean_cv['f1_score'] = np.mean(f1_score_list)  ; std_cv['f1_score'] = np.std(acc_list)\n",
    "        print('Validation set: Cross-validation average = {}'.format(mean_cv))\n",
    "        print('Validation set: Cross-validation std = {:}'.format(std_cv))\n",
    "           \n",
    "    else:       \n",
    "        #      Resample data\n",
    "        if under_samp_meth != None:\n",
    "            us, x_train, y_train, ind_us = under_samp(method, seed, r_Maj_over_minas, x_train, y_train)\n",
    "        if over_samp_meth != None:\n",
    "            os, n_fail_y_train, x_train, y_train = over_samp(method, seed, r_Maj_over_minas, x_train, y_train)    \n",
    "        clf = ens_clf(ml_clf, seed_used, feat_list, x_train, y_train, 100)    \n",
    "        # Make predictions on train set\n",
    "        y_pred_train = clf.predict(x_train)\n",
    "        out['y_pred_train'] = y_pred_train\n",
    "        # keep probabilities for the positive outcome only\n",
    "        try:\n",
    "            y_proba_train = clf.predict_proba(x_train)[:,1]\n",
    "        except:\n",
    "            y_proba_train = 'Classifier has no \"predict_proba\" method'\n",
    "            print('Train set: Classifier ' + str(ml_clf) + ' has no \"predict_proba\" method')\n",
    "        out['y_proba_train'] = y_proba_train\n",
    "        \n",
    "        # Compute performance metrics on train set\n",
    "        perf_dict['train'], traces_dict['train'] = compute_metrics(y_train, y_pred_train, y_proba_train)\n",
    "        # Print train metrics\n",
    "        print('train_perf = \\n{}'.format(perf_dict['train']))\n",
    "        if plot_cfm :         \n",
    "            plot_confusion_matrix(y_train, y_pred_train, classes=['0', '1'], title='Confusion matrix on TRAIN SET')\n",
    "            plt.show()\n",
    "                \n",
    "    # Make predictions on test set\n",
    "    y_pred_test = clf.predict(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    try:           \n",
    "        y_proba_test = clf.predict_proba(x_test)[:,1]\n",
    "    except:\n",
    "        y_proba_test = 'Classifier has no \"predict_proba\" method'\n",
    "        print('Test set: Classifier ' + str(ml_clf) + ' has no \"predict_proba\" method')                       \n",
    "    # Compute performance metrics on test set\n",
    "    perf_dict['test'], traces_dict['test'] = compute_metrics(y_test, y_pred_test, y_proba_test)\n",
    "\n",
    "    # Print test metrics\n",
    "    print('test_perf = \\n{}'.format(perf_dict['test']))\n",
    "\n",
    "    # Plot Confusion matrices\n",
    "    if plot_cfm :         \n",
    "        plot_confusion_matrix(y_test, y_pred_test, classes=['0', '1'], title='Confusion matrix on TEST SET')\n",
    "        plt.show()\n",
    " \n",
    "    # Plot PR Curve\n",
    "    if (plot_prc == 1):\n",
    "        fig = plotPRCurve(traces_dict, perf_dict)\n",
    "    else:\n",
    "        fig = None\n",
    "        \n",
    "    out['feat_list'] = feat_list\n",
    "    out['clf'] = clf\n",
    "    out['x_train'] = x_train\n",
    "    out['x_test'] = x_test    \n",
    "    out['y_pred_test'] = y_pred_test\n",
    "    out['y_proba_test'] = y_proba_test\n",
    "    out['fig'] = fig\n",
    "    \n",
    "    return out, perf_dict, traces_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test6: Bag of Classifiers with  <=0 samples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "out_t6, perf_dict_t6, traces_dict_t6 = myPipeline(df, 2, test_size=0.5, feat_sel_mode='not_pca', cross_val_meth='cv', under_samp_meth=None, \n",
    "               over_samp_meth=None, r_Majas_over_min=1, r_Maj_over_minas=1, ml_clf= 'CNB', plot_prc=True, plot_cfm=True )\n",
    "t2 = time.time()\n",
    "elapsedTime = t2-t1\n",
    "print('\\nelapsed time = {}'.format(np.round(elapsedTime,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test1: single classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "t1 = time.time()\n",
    "out_t1, perf_dict_t1, traces_dict_t1 = myPipeline(df, 2, test_size=0.5, feat_sel_mode='not_pca', under_samp_meth=None, \n",
    "               over_samp_meth=None, r_Majas_over_min=1, r_Maj_over_minas=1, ml_clf='RF', plot_prc=True, plot_cfm=True )\n",
    "t2 = time.time()\n",
    "elapsedTime = t2-t1\n",
    "print('\\nelapsed time = {}'.format(np.round(elapsedTime,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test2: Ensemble of classifiers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "out_t2, perf_dict_t2, traces_dict_t2 = myPipeline(df, 42, test_size=0.5, feat_sel_mode='not_pca', under_samp_meth=None, \n",
    "               over_samp_meth=None, r_Majas_over_min=1, r_Maj_over_minas=1, ml_clf= ['xgb', 'MLP'], plot_prc=True, plot_cfm=True )\n",
    "t2 = time.time()\n",
    "elapsedTime = t2-t1\n",
    "print('\\nelapsed time = {}'.format(np.round(elapsedTime,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dict = {'t1':traces_dict_t1['test'],'t2':traces_dict_t2['test']}\n",
    "pe_dict = {'t1':perf_dict_t1['test'],'t2':perf_dict_t2['test']}\n",
    "plotPRCurve(tr_dict, pe_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
